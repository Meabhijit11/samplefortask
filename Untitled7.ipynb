{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "694dd854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9998d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"dataset.csv\")  # Replace \"your_dataset.csv\" with the actual file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8853d4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to datetime objects\n",
    "data['start_date'] = pd.to_datetime(data['start_date'])\n",
    "data['end_date'] = pd.to_datetime(data['end_date'])\n",
    "\n",
    "# Extract relevant time features\n",
    "data['day_of_week'] = data['start_date'].dt.day_of_week\n",
    "data['hour_of_day'] = data['start_time_hour']\n",
    "data['15_min_intervals'] = data['start_time_hour'] * 4 + data['start_time_minute'] / 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b60db0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['tripCount'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14720/1098483413.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Define features and target variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tripCount'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tripCount'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4904\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[1;36m1.0\u001b[0m     \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4905\u001b[0m         \"\"\"\n\u001b[1;32m-> 4906\u001b[1;33m         return super().drop(\n\u001b[0m\u001b[0;32m   4907\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4908\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4148\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4149\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4150\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4152\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   4183\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4184\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4185\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4186\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6015\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6017\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6018\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6019\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['tripCount'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns\n",
    "data.drop(['start_date', 'end_date', 'start_time_hour', 'start_time_minute', 'end_time_hour', 'end_time_minute'], axis=1, inplace=True)\n",
    "\n",
    "# Define features and target variable\n",
    "features = data.drop('tripCount', axis=1)\n",
    "target = data['tripCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba706ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9a4e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8793c5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression model\n",
    "linear_reg_model = LinearRegression()\n",
    "linear_reg_model.fit(X_train_scaled, y_train)\n",
    "linear_reg_predictions = linear_reg_model.predict(X_test_scaled)\n",
    "\n",
    "# XGBoost model\n",
    "xgb_model = XGBRegressor()\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "xgb_predictions = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "# ARIMA model\n",
    "arima_model = ARIMA(y_train, order=(5,1,0))  # You may need to optimize the order parameter\n",
    "arima_model_fit = arima_model.fit()\n",
    "arima_predictions = arima_model_fit.forecast(steps=len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46db2e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c37690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Neural Network model\n",
    "# nn_model = Sequential([\n",
    "#     Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "#     Dense(32, activation='relu'),\n",
    "#     Dense(1)\n",
    "# ])\n",
    "\n",
    "# nn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# # Use early stopping to prevent overfitting\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# nn_model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# nn_predictions = nn_model.predict(X_test_scaled).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141007f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the models\n",
    "linear_reg_rmse = mean_squared_error(y_test, linear_reg_predictions, squared=False)\n",
    "xgb_rmse = mean_squared_error(y_test, xgb_predictions, squared=False)\n",
    "arima_rmse = mean_squared_error(y_test, arima_predictions, squared=False)\n",
    "# nn_rmse = mean_squared_error(y_test, nn_predictions, squared=False)\n",
    "\n",
    "print(\"Linear Regression RMSE:\", linear_reg_rmse)\n",
    "print(\"XGBoost RMSE:\", xgb_rmse)\n",
    "print(\"ARIMA RMSE:\", arima_rmse)\n",
    "# print(\"Neural Network RMSE:\", nn_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42be0b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "501247d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meabh\\anaconda3\\envs\\py38\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:590: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  warnings.warn('An unsupported index was provided and will be'\n",
      "C:\\Users\\meabh\\anaconda3\\envs\\py38\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:590: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  warnings.warn('An unsupported index was provided and will be'\n",
      "C:\\Users\\meabh\\anaconda3\\envs\\py38\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:590: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  warnings.warn('An unsupported index was provided and will be'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE: 136.65071292457435\n",
      "XGBoost RMSE: 74.0464069410539\n",
      "ARIMA RMSE: 219.3941361057772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meabh\\anaconda3\\envs\\py38\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:390: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  warnings.warn('No supported index is available.'\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"dataset.csv\")  # Replace \"your_dataset.csv\" with the actual file name\n",
    "\n",
    "# Convert date columns to datetime objects\n",
    "data['start_date'] = pd.to_datetime(data['start_date'])\n",
    "data['end_date'] = pd.to_datetime(data['end_date'])\n",
    "\n",
    "# Extract relevant time features\n",
    "data['day_of_week'] = data['start_date'].dt.day_of_week\n",
    "data['hour_of_day'] = data['start_time_hour']\n",
    "data['15_min_intervals'] = data['start_time_hour'] * 4 + data['start_time_minute'] / 15\n",
    "\n",
    "# Aggregate demand for each 15-minute interval\n",
    "demand_data = data.groupby('15_min_intervals').size().reset_index(name='tripCount')\n",
    "\n",
    "# Define features and target variable\n",
    "features = demand_data.drop('tripCount', axis=1)\n",
    "target = demand_data['tripCount']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Linear Regression model\n",
    "linear_reg_model = LinearRegression()\n",
    "linear_reg_model.fit(X_train_scaled, y_train)\n",
    "linear_reg_predictions = linear_reg_model.predict(X_test_scaled)\n",
    "\n",
    "# XGBoost model\n",
    "xgb_model = XGBRegressor()\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "xgb_predictions = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "# ARIMA model\n",
    "arima_model = ARIMA(y_train, order=(5,1,0))  # You may need to optimize the order parameter\n",
    "arima_model_fit = arima_model.fit()\n",
    "arima_predictions = arima_model_fit.forecast(steps=len(X_test))\n",
    "\n",
    "# Neural Network model\n",
    "# nn_model = Sequential([\n",
    "#     Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "#     Dense(32, activation='relu'),\n",
    "#     Dense(1)\n",
    "# ])\n",
    "\n",
    "# nn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# # Use early stopping to prevent overfitting\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# nn_model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# nn_predictions = nn_model.predict(X_test_scaled).flatten()\n",
    "\n",
    "# Evaluate the models\n",
    "linear_reg_rmse = mean_squared_error(y_test, linear_reg_predictions, squared=False)\n",
    "xgb_rmse = mean_squared_error(y_test, xgb_predictions, squared=False)\n",
    "arima_rmse = mean_squared_error(y_test, arima_predictions, squared=False)\n",
    "# nn_rmse = mean_squared_error(y_test, nn_predictions, squared=False)\n",
    "\n",
    "print(\"Linear Regression RMSE:\", linear_reg_rmse)\n",
    "print(\"XGBoost RMSE:\", xgb_rmse)\n",
    "print(\"ARIMA RMSE:\", arima_rmse)\n",
    "# print(\"Neural Network RMSE:\", nn_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a98112c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Predictions: [71.70160392 92.54192646]\n",
      "XGBoost Predictions: [10.207976 13.151566]\n",
      "ARIMA Predictions: 1073    244.058196\n",
      "1074    298.302079\n",
      "Name: predicted_mean, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meabh\\anaconda3\\envs\\py38\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:390: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  warnings.warn('No supported index is available.'\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'features' contains the relevant features used during training\n",
    "# Adjust the feature_columns list accordingly if needed\n",
    "feature_columns = ['tripDistance', 'tripSpeed', 'tripDuration', 'paymentType', 'day_of_week', 'hour_of_day', '15_min_intervals', 'startLatitude', 'startLongitude', 'endLatitude', 'endLongitude']\n",
    "\n",
    "# Assuming 'features' contains the relevant features used during training\n",
    "# Get the actual feature columns from X_train\n",
    "feature_columns = X_train.columns.tolist()\n",
    "\n",
    "# Fit the scaler to the training features\n",
    "scaler.fit(X_train[feature_columns])\n",
    "\n",
    "# Create sample data for prediction\n",
    "sample_data = pd.DataFrame({\n",
    "    'tripDistance': [7.2, 4.8],\n",
    "    'tripSpeed': [16.5, 19.8],\n",
    "    'tripDuration': [25, 15],\n",
    "    'paymentType': [1, 0],\n",
    "    'start_date': ['01-02-2022', '01-02-2022'],\n",
    "    'end_date': ['01-02-2022', '01-02-2022'],\n",
    "    'start_time_hour': [12, 14],\n",
    "    'start_time_minute': [30, 45],\n",
    "    'end_time_hour': [12, 15],\n",
    "    'end_time_minute': [55, 0],\n",
    "    'startLatitude': [23.045678, 22.978765],\n",
    "    'startLongitude': [72.589012, 72.540123],\n",
    "    'endLatitude': [23.065432, 22.988765],\n",
    "    'endLongitude': [72.598765, 72.550987]\n",
    "})\n",
    "\n",
    "# Convert date columns to datetime objects\n",
    "sample_data['start_date'] = pd.to_datetime(sample_data['start_date'])\n",
    "sample_data['end_date'] = pd.to_datetime(sample_data['end_date'])\n",
    "\n",
    "# Extract relevant time features\n",
    "sample_data['day_of_week'] = sample_data['start_date'].dt.day_of_week\n",
    "sample_data['hour_of_day'] = sample_data['start_time_hour']\n",
    "sample_data['15_min_intervals'] = sample_data['start_time_hour'] * 4 + sample_data['start_time_minute'] / 15\n",
    "\n",
    "# Drop unnecessary columns\n",
    "sample_data.drop(['start_date', 'end_date', 'start_time_hour', 'start_time_minute', 'end_time_hour', 'end_time_minute'], axis=1, inplace=True)\n",
    "\n",
    "# Standardize the features using the same scaler from training\n",
    "sample_data_scaled = scaler.transform(sample_data[feature_columns])\n",
    "\n",
    "# Make predictions using the trained models\n",
    "linear_reg_predictions = linear_reg_model.predict(sample_data_scaled)\n",
    "xgb_predictions = xgb_model.predict(sample_data_scaled)\n",
    "arima_predictions = arima_model_fit.forecast(steps=len(sample_data_scaled))\n",
    "# nn_predictions = nn_model.predict(sample_data_scaled).flatten()\n",
    "\n",
    "# Print the predictions\n",
    "print(\"Linear Regression Predictions:\", linear_reg_predictions)\n",
    "print(\"XGBoost Predictions:\", xgb_predictions)\n",
    "print(\"ARIMA Predictions:\", arima_predictions)\n",
    "# print(\"Neural Network Predictions:\", nn_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897e59f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa25e8f7",
   "metadata": {},
   "source": [
    "#######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9f3959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c4d790e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Mean Squared Error: 6.64107991958541e-23\n",
      "XGBoost Mean Squared Error: 0.12865236644731054\n",
      "ARIMA Mean Squared Error: 511.47982120952094\n",
      "DNN Mean Squared Error: 0.16122849794259883\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('dataset.csv')  # Replace 'your_dataset.csv' with the actual file path\n",
    "\n",
    "# Data Preprocessing\n",
    "df['start_datetime'] = pd.to_datetime(df['start_date'] + ' ' + df['start_time_hour'].astype(str) + ':' + df['start_time_minute'].astype(str))\n",
    "df['end_datetime'] = pd.to_datetime(df['start_date'] + ' ' + df['end_time_hour'].astype(str) + ':' + df['end_time_minute'].astype(str))\n",
    "df['day_of_week'] = df['start_datetime'].dt.dayofweek\n",
    "\n",
    "# Feature Engineering\n",
    "df['demand'] = 1  # Assume each record represents one demand\n",
    "df['lag_demand_15min'] = df.groupby('start_datetime')['demand'].shift(1)\n",
    "df['lag_demand_30min'] = df.groupby('start_datetime')['demand'].shift(2)\n",
    "df['lag_demand_60min'] = df.groupby('start_datetime')['demand'].shift(4)\n",
    "\n",
    "# Aggregation by time intervals (you can explore more granular or coarser intervals)\n",
    "aggregated_data = df.resample('15T', on='start_datetime').sum()\n",
    "\n",
    "# Define features and target variable\n",
    "features = ['tripDistance', 'tripSpeed', 'tripDuration', 'tripFare', 'paymentType', 'day_of_week',\n",
    "            'lag_demand_15min', 'lag_demand_30min', 'lag_demand_60min']\n",
    "target = 'demand'\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(aggregated_data) * 0.8)\n",
    "train, test = aggregated_data.iloc[:train_size], aggregated_data.iloc[train_size:]\n",
    "\n",
    "# Linear Regression\n",
    "linear_reg_model = LinearRegression()\n",
    "linear_reg_model.fit(train[features], train[target])\n",
    "linear_reg_predictions = linear_reg_model.predict(test[features])\n",
    "linear_reg_mse = mean_squared_error(test[target], linear_reg_predictions)\n",
    "print(f'Linear Regression Mean Squared Error: {linear_reg_mse}')\n",
    "\n",
    "# XGBoost\n",
    "xgboost_model = XGBRegressor()\n",
    "xgboost_model.fit(train[features], train[target])\n",
    "xgboost_predictions = xgboost_model.predict(test[features])\n",
    "xgboost_mse = mean_squared_error(test[target], xgboost_predictions)\n",
    "print(f'XGBoost Mean Squared Error: {xgboost_mse}')\n",
    "\n",
    "# ARIMA\n",
    "arima_model = ARIMA(train[target], order=(5, 1, 0))\n",
    "arima_model_fit = arima_model.fit()\n",
    "arima_predictions = arima_model_fit.predict(start=len(train), end=len(train) + len(test) - 1, typ='levels')\n",
    "arima_mse = mean_squared_error(test[target], arima_predictions)\n",
    "print(f'ARIMA Mean Squared Error: {arima_mse}')\n",
    "\n",
    "# DNN\n",
    "dnn_model = Pipeline([\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), features)\n",
    "    ])),\n",
    "    ('regressor', MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000))\n",
    "])\n",
    "dnn_model.fit(train[features], train[target])\n",
    "dnn_predictions = dnn_model.predict(test[features])\n",
    "dnn_mse = mean_squared_error(test[target], dnn_predictions)\n",
    "print(f'DNN Mean Squared Error: {dnn_mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b442a5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Predictions: [-8.05014421e-13]\n",
      "XGBoost Predictions: [1.0000703]\n",
      "ARIMA Predictions: start_datetime\n",
      "2022-01-01 06:45:00    0.999998\n",
      "Freq: 15T, dtype: float64\n",
      "DNN Predictions: [0.98679348]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is your original dataset\n",
    "# Assuming you have trained models named linear_reg_model, xgboost_model, arima_model_fit, and dnn_model\n",
    "\n",
    "# Sample data for prediction\n",
    "sample_data = pd.DataFrame({\n",
    "    'tripDistance': [5.0],\n",
    "    'tripSpeed': [20.0],\n",
    "    'tripDuration': [15.0],\n",
    "    'tripFare': [50],\n",
    "    'paymentType': [1],\n",
    "    'start_date': ['2022-01-03'],\n",
    "    'start_time_hour': [12],\n",
    "    'start_time_minute': [0],\n",
    "})\n",
    "\n",
    "# Data Preprocessing for the sample data\n",
    "sample_data['start_datetime'] = pd.to_datetime(sample_data['start_date'] + ' ' + sample_data['start_time_hour'].astype(str) + ':' + sample_data['start_time_minute'].astype(str))\n",
    "sample_data['day_of_week'] = sample_data['start_datetime'].dt.dayofweek\n",
    "sample_data['demand'] = 1  # Assuming each record represents one demand\n",
    "sample_data['lag_demand_15min'] = sample_data['demand'].shift(1)\n",
    "sample_data['lag_demand_30min'] = sample_data['demand'].shift(2)\n",
    "sample_data['lag_demand_60min'] = sample_data['demand'].shift(4)\n",
    "\n",
    "# Aggregation for the sample data (you may need to adjust this based on your time intervals)\n",
    "sample_data_aggregated = sample_data.resample('15T', on='start_datetime').sum()\n",
    "\n",
    "# Define features for prediction\n",
    "prediction_features = ['tripDistance', 'tripSpeed', 'tripDuration', 'tripFare', 'paymentType', 'day_of_week',\n",
    "                       'lag_demand_15min', 'lag_demand_30min', 'lag_demand_60min']\n",
    "\n",
    "# Make predictions using the trained models\n",
    "linear_reg_predictions = linear_reg_model.predict(sample_data_aggregated[prediction_features].fillna(0))\n",
    "xgboost_predictions = xgboost_model.predict(sample_data_aggregated[prediction_features].fillna(0))\n",
    "arima_predictions = arima_model_fit.predict(start=len(sample_data_aggregated), end=len(sample_data_aggregated), typ='levels')\n",
    "dnn_predictions = dnn_model.predict(sample_data_aggregated[prediction_features].fillna(0))\n",
    "\n",
    "# Display the predictions\n",
    "print(\"Linear Regression Predictions:\", linear_reg_predictions)\n",
    "print(\"XGBoost Predictions:\", xgboost_predictions)\n",
    "print(\"ARIMA Predictions:\", arima_predictions)\n",
    "print(\"DNN Predictions:\", dnn_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eaf32c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e2fc38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563aaad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa95285f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3591f574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbb74f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MSE: 14516.244450586084\n",
      "Random Forest MSE: 15095.332199964063\n",
      "XGBoost MSE: 14776.967577485462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meabh\\anaconda3\\envs\\py38\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:590: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  warnings.warn('An unsupported index was provided and will be'\n",
      "C:\\Users\\meabh\\anaconda3\\envs\\py38\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:590: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  warnings.warn('An unsupported index was provided and will be'\n",
      "C:\\Users\\meabh\\anaconda3\\envs\\py38\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:590: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  warnings.warn('An unsupported index was provided and will be'\n",
      "C:\\Users\\meabh\\anaconda3\\envs\\py38\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:390: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  warnings.warn('No supported index is available.'\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\pandas\\core\\indexes\\range.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    384\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 0 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9392/2215117908.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[0marima_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mARIMA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[0marima_fit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marima_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m \u001b[0marima_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marima_fit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforecast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ARIMA MSE:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marima_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[1;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m         \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\pandas\\core\\indexes\\range.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    385\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    388\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load the dataset\n",
    "# Assuming the dataset is stored in a CSV file named 'taxi_data.csv'\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Convert date columns to datetime format\n",
    "df['start_date'] = pd.to_datetime(df['start_date'])\n",
    "\n",
    "# Create a new column for 15-minute intervals\n",
    "df['15min_interval'] = df['start_date'].dt.floor('15T')\n",
    "\n",
    "# Aggregate data to get the count of trips within each 15-minute interval\n",
    "demand_data = df.groupby('15min_interval').size().reset_index(name='demand')\n",
    "\n",
    "# Merge demand data back into the original dataset\n",
    "df = pd.merge(df, demand_data, left_on='15min_interval', right_on='15min_interval', how='left')\n",
    "\n",
    "# Extract additional time-related features\n",
    "df['day_of_week'] = df['start_date'].dt.dayofweek\n",
    "df['hour_of_day'] = df['start_date'].dt.hour\n",
    "\n",
    "# Define features and target variable\n",
    "features = ['tripDistance', 'tripSpeed', 'tripDuration', 'startLatitude', 'startLongitude', \n",
    "            'endLatitude', 'endLongitude', 'day_of_week', 'hour_of_day']\n",
    "\n",
    "target = 'demand'\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Linear Regression\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "linear_pred = linear_model.predict(X_test)\n",
    "print('Linear Regression MSE:', mean_squared_error(y_test, linear_pred))\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestRegressor()\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "print('Random Forest MSE:', mean_squared_error(y_test, rf_pred))\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = XGBRegressor()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "print('XGBoost MSE:', mean_squared_error(y_test, xgb_pred))\n",
    "\n",
    "# ARIMA\n",
    "arima_model = ARIMA(y_train, order=(5, 1, 0))\n",
    "arima_fit = arima_model.fit()\n",
    "arima_pred = arima_fit.forecast(steps=len(X_test))[0]\n",
    "print('ARIMA MSE:', mean_squared_error(y_test, arima_pred))\n",
    "\n",
    "# Deep Neural Network (DNN)\n",
    "# Feature scaling\n",
    "# scaler = StandardScaler()\n",
    "# ct = ColumnTransformer([('scaler', scaler, features)])\n",
    "# dnn_model = Sequential([\n",
    "#     Dense(64, activation='relu', input_shape=(len(features),)),\n",
    "#     Dense(32, activation='relu'),\n",
    "#     Dense(1)\n",
    "# ])\n",
    "# dnn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Create pipeline\n",
    "# dnn_pipeline = Pipeline([('preprocessor', ct), ('model', dnn_model)])\n",
    "# dnn_pipeline.fit(X_train, y_train, model__epochs=10, model__batch_size=32, model__verbose=0)\n",
    "# dnn_pred = dnn_pipeline.predict(X_test)\n",
    "# print('DNN MSE:', mean_squared_error(y_test, dnn_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94fc42d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
